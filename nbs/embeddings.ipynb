{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from ftfy import fix_text\n",
    "import re\n",
    "import os\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim import downloader as api\n",
    "from gensim.models import FastText\n",
    "import warnings\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = None\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL_REGEX = re.compile(\n",
    "    r\"(?:^|(?<=[^\\w@.)]))([\\w+-](\\.(?!\\.))?)*?[\\w+-]@(?:\\w-?)*?\\w+(\\.([a-z]{2,})){1,3}(?:$|(?=\\b))\",\n",
    "    flags=re.IGNORECASE | re.UNICODE,\n",
    ")\n",
    "\n",
    "PHONE_REGEX = re.compile(\n",
    "    r\"(?:^|(?<=[^\\w)]))(\\+?1[ .-]?)?(\\(?\\d{3}\\)?[ .-]?)?(\\d{3}[ .-]?\\d{4})(\\s?(?:ext\\.?|[#x-])\\s?\\d{2,6})?(?:$|(?=\\W))\"\n",
    ")\n",
    "\n",
    "MULTI_WHITESPACE_TO_ONE_REGEX = re.compile(r\"\\s+\")\n",
    "\n",
    "URL_REGEX = re.compile(\n",
    "    r\"(?:^|(?<![\\w\\/\\.]))\"\n",
    "    # protocol identifier\n",
    "    # r\"(?:(?:https?|ftp)://)\"  <-- alt?\n",
    "    r\"(?:(?:https?:\\/\\/|ftp:\\/\\/|www\\d{0,3}\\.))\"\n",
    "    # user:pass authentication\n",
    "    r\"(?:\\S+(?::\\S*)?@)?\" r\"(?:\"\n",
    "    # IP address exclusion\n",
    "    # private & local networks\n",
    "    r\"(?!(?:10|127)(?:\\.\\d{1,3}){3})\"\n",
    "    r\"(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})\"\n",
    "    r\"(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})\"\n",
    "    # IP address dotted notation octets\n",
    "    # excludes loopback network 0.0.0.0\n",
    "    # excludes reserved space >= 224.0.0.0\n",
    "    # excludes network & broadcast addresses\n",
    "    # (first & last IP address of each class)\n",
    "    r\"(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])\"\n",
    "    r\"(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}\"\n",
    "    r\"(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))\"\n",
    "    r\"|\"\n",
    "    # host name\n",
    "    r\"(?:(?:[a-z\\\\u00a1-\\\\uffff0-9]-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)\"\n",
    "    # domain name\n",
    "    r\"(?:\\.(?:[a-z\\\\u00a1-\\\\uffff0-9]-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)*\"\n",
    "    # TLD identifier\n",
    "    r\"(?:\\.(?:[a-z\\\\u00a1-\\\\uffff]{2,}))\" r\")\"\n",
    "    # port number\n",
    "    r\"(?::\\d{2,5})?\"\n",
    "    # resource path\n",
    "    r\"(?:\\/[^\\)\\]\\}\\s]*)?\",\n",
    "    # r\"(?:$|(?![\\w?!+&\\/\\)]))\",\n",
    "    # @jfilter: I removed the line above from the regex because I don't understand what it is used for, maybe it was useful?\n",
    "    # But I made sure that it does not include ), ] and } in the URL.\n",
    "    flags=re.UNICODE | re.IGNORECASE,\n",
    ")\n",
    "\n",
    "strange_double_quotes = [\n",
    "    \"«\",\n",
    "    \"‹\",\n",
    "    \"»\",\n",
    "    \"›\",\n",
    "    \"„\",\n",
    "    \"“\",\n",
    "    \"‟\",\n",
    "    \"”\",\n",
    "    \"❝\",\n",
    "    \"❞\",\n",
    "    \"❮\",\n",
    "    \"❯\",\n",
    "    \"〝\",\n",
    "    \"〞\",\n",
    "    \"〟\",\n",
    "    \"＂\",\n",
    "]\n",
    "strange_single_quotes = [\"‘\", \"‛\", \"’\", \"❛\", \"❜\", \"`\", \"´\", \"‘\", \"’\"]\n",
    "\n",
    "DOUBLE_QUOTE_REGEX = re.compile(\"|\".join(strange_double_quotes))\n",
    "SINGLE_QUOTE_REGEX = re.compile(\"|\".join(strange_single_quotes))\n",
    "HASHTAG_REGEX = re.compile(\"(?:^|\\s)[＃#]{1}(\\w+)\", re.UNICODE)\n",
    "MENTION_REGEX = re.compile(\"(?:^|\\s)[＠ @]{1}([^\\s#<>[\\]|{}]+)\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_strange_quotes(text):\n",
    "    text = SINGLE_QUOTE_REGEX.sub(\"'\", text)\n",
    "    text = DOUBLE_QUOTE_REGEX.sub('\"', text)\n",
    "    return text\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    text = MULTI_WHITESPACE_TO_ONE_REGEX.sub(\" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def replace_urls(text, replace_with=\"<URL>\"):\n",
    "    return URL_REGEX.sub(replace_with, text)\n",
    "\n",
    "def replace_emails(text, replace_with=\"<EMAIL>\"):\n",
    "    return EMAIL_REGEX.sub(replace_with, text)\n",
    "\n",
    "def replace_phone_numbers(text, replace_with=\"<PHONE>\"):\n",
    "    return PHONE_REGEX.sub(replace_with, text)\n",
    "\n",
    "def replace_hashtag(text, replace_with=''):\n",
    "    return HASHTAG_REGEX.sub(replace_with, text)\n",
    "\n",
    "def replace_mentions(text, replace_with=''):\n",
    "    return MENTION_REGEX.sub(replace_with, text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = fix_text(text)\n",
    "    text = fix_strange_quotes(text)\n",
    "    text = replace_urls(text, replace_with='')\n",
    "    text = replace_emails(text, replace_with='')\n",
    "    text = replace_phone_numbers(text, replace_with='')\n",
    "    text = replace_hashtag(text)\n",
    "    text = replace_mentions(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data_paths = [\n",
    "    Path('../../full-corpus.csv'), \n",
    "    Path('../../TextEmotion.csv'), \n",
    "    Path('../data/test.csv'),\n",
    "    Path('../data/train.csv')\n",
    "]\n",
    "data_columns = [\n",
    "    'TweetText',\n",
    "    'content',\n",
    "    'text',\n",
    "    'text'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb0cc48d35546ed96fac66182f7a611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423b1744754644689c8337d371bd80e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cleaning full-corpus.csv', max=5113.0, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d94a1cc16c420bbdfac59953098e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cleaning TextEmotion.csv', max=40000.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09552a93f02445d780e7f5c70f72b2d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cleaning test.csv', max=3534.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4811a17a088f4aa1a586ba92461c91f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cleaning train.csv', max=27480.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_tweets = []\n",
    "for i, data_path in enumerate(tqdm(tweet_data_paths)):\n",
    "    df: pd.DataFrame = pd.read_csv(data_path)\n",
    "    df.dropna(inplace=True)\n",
    "    tqdm.pandas(desc=f'Cleaning {data_path.name}')\n",
    "    df[data_columns[i]] = df[data_columns[i]].progress_apply(lambda x: clean_text(x))\n",
    "    processed_tweets += df[data_columns[i]].tolist()\n",
    "processed_tweets = [x.split() for x in processed_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['now', 'swype', 'iphone', 'crack.', 'iphone'], ['adding', 'carrier', 'support', 'iphone', '4s', '(just', 'announced)'], ['hilarious', 'video', '-', 'guy', 'duet', \"'s\", 'siri.', 'pretty', 'sums', 'love', 'affair!'], ['easy', 'switch', 'iphone.', 'see', 'ya!'], ['i', 'realized', 'reason', 'i', 'got', 'twitter', 'ios5', 'thanks']]\n"
     ]
    }
   ],
   "source": [
    "print(processed_tweets[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on text8\n",
    "dataset = api.load('text8')\n",
    "ft_model = FastText(min_count=3, size=300, min_n=2, max_n=5, iter=10, workers=os.cpu_count())\n",
    "ft_model.build_vocab(dataset)\n",
    "total_words = ft_model.corpus_total_words\n",
    "ft_model.train(dataset, total_words=total_words, epochs=ft_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path('../embeddings/fasttext/twitter/twitter_ft.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-30 17:53:26,992 : INFO : loading FastText object from ..\\embeddings\\fasttext\\twitter\\twitter_ft.model\n",
      "2020-05-30 17:53:27,213 : INFO : loading wv recursively from ..\\embeddings\\fasttext\\twitter\\twitter_ft.model.wv.* with mmap=None\n",
      "2020-05-30 17:53:27,214 : INFO : loading vectors from ..\\embeddings\\fasttext\\twitter\\twitter_ft.model.wv.vectors.npy with mmap=None\n",
      "2020-05-30 17:53:27,265 : INFO : loading vectors_vocab from ..\\embeddings\\fasttext\\twitter\\twitter_ft.model.wv.vectors_vocab.npy with mmap=None\n",
      "2020-05-30 17:53:27,314 : INFO : loading vectors_ngrams from ..\\embeddings\\fasttext\\twitter\\twitter_ft.model.wv.vectors_ngrams.npy with mmap=None\n",
      "2020-05-30 17:53:28,343 : INFO : setting ignored attribute vectors_norm to None\n",
      "2020-05-30 17:53:28,343 : INFO : setting ignored attribute vectors_vocab_norm to None\n",
      "2020-05-30 17:53:28,344 : INFO : setting ignored attribute vectors_ngrams_norm to None\n",
      "2020-05-30 17:53:28,344 : INFO : setting ignored attribute buckets_word to None\n",
      "2020-05-30 17:53:28,344 : INFO : loading vocabulary recursively from ..\\embeddings\\fasttext\\twitter\\twitter_ft.model.vocabulary.* with mmap=None\n",
      "2020-05-30 17:53:28,345 : INFO : loading trainables recursively from ..\\embeddings\\fasttext\\twitter\\twitter_ft.model.trainables.* with mmap=None\n",
      "2020-05-30 17:53:28,345 : INFO : loading syn1neg from ..\\embeddings\\fasttext\\twitter\\twitter_ft.model.trainables.syn1neg.npy with mmap=None\n",
      "2020-05-30 17:53:28,396 : INFO : loading vectors_vocab_lockf from ..\\embeddings\\fasttext\\twitter\\twitter_ft.model.trainables.vectors_vocab_lockf.npy with mmap=None\n",
      "2020-05-30 17:53:28,445 : INFO : loading vectors_ngrams_lockf from ..\\embeddings\\fasttext\\twitter\\twitter_ft.model.trainables.vectors_ngrams_lockf.npy with mmap=None\n",
      "2020-05-30 17:53:29,396 : INFO : loaded ..\\embeddings\\fasttext\\twitter\\twitter_ft.model\n",
      "2020-05-30 17:53:29,544 : INFO : collecting all words and their counts\n",
      "2020-05-30 17:53:29,544 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-05-30 17:53:29,563 : INFO : PROGRESS: at sentence #10000, processed 79253 words, keeping 23225 word types\n",
      "2020-05-30 17:53:29,581 : INFO : PROGRESS: at sentence #20000, processed 155876 words, keeping 36489 word types\n",
      "2020-05-30 17:53:29,600 : INFO : PROGRESS: at sentence #30000, processed 229160 words, keeping 47768 word types\n",
      "2020-05-30 17:53:29,617 : INFO : PROGRESS: at sentence #40000, processed 300898 words, keeping 57999 word types\n",
      "2020-05-30 17:53:29,635 : INFO : PROGRESS: at sentence #50000, processed 373311 words, keeping 63295 word types\n",
      "2020-05-30 17:53:29,653 : INFO : PROGRESS: at sentence #60000, processed 445507 words, keeping 63984 word types\n",
      "2020-05-30 17:53:29,670 : INFO : PROGRESS: at sentence #70000, processed 517488 words, keeping 64579 word types\n",
      "2020-05-30 17:53:29,682 : INFO : collected 64916 word types from a corpus of 561505 raw words and 76127 sentences\n",
      "2020-05-30 17:53:29,683 : INFO : Updating model with new vocabulary\n",
      "2020-05-30 17:53:29,726 : INFO : New added 18420 unique words (22% of original 83336) and increased the count of 18420 pre-existing words (22% of original 83336)\n",
      "2020-05-30 17:53:29,829 : INFO : deleting the raw counts dictionary of 64916 items\n",
      "2020-05-30 17:53:29,831 : INFO : sample=0.001 downsamples 66 most-common words\n",
      "2020-05-30 17:53:29,831 : INFO : downsampling leaves estimated 899229 word corpus (184.8% of prior 486683)\n",
      "2020-05-30 17:53:31,071 : INFO : estimated required memory for 109392 words, 244165 buckets and 300 dimensions: 639507624 bytes\n",
      "2020-05-30 17:53:31,079 : INFO : updating layer weights\n",
      "2020-05-30 17:53:35,673 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2020-05-30 17:53:35,673 : INFO : training model with 12 workers on 109392 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-05-30 17:53:36,714 : INFO : EPOCH 1 - PROGRESS: at 51.63% words, 234321 words/s, in_qsize 22, out_qsize 1\n",
      "2020-05-30 17:53:37,117 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-30 17:53:37,153 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-30 17:53:37,164 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-30 17:53:37,184 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-30 17:53:37,188 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-30 17:53:37,206 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-30 17:53:37,214 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-30 17:53:37,219 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-30 17:53:37,227 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-30 17:53:37,250 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-30 17:53:37,261 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-30 17:53:37,265 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-30 17:53:37,266 : INFO : EPOCH - 1 : training on 561505 raw words (461330 effective words) took 1.6s, 294946 effective words/s\n",
      "2020-05-30 17:53:38,398 : INFO : EPOCH 2 - PROGRESS: at 55.19% words, 227235 words/s, in_qsize 22, out_qsize 1\n",
      "2020-05-30 17:53:38,787 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-30 17:53:38,844 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-30 17:53:38,877 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-30 17:53:38,886 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-30 17:53:38,890 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-30 17:53:38,942 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-30 17:53:38,945 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-30 17:53:38,952 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-30 17:53:38,961 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-30 17:53:38,964 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-30 17:53:39,012 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-30 17:53:39,016 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-30 17:53:39,017 : INFO : EPOCH - 2 : training on 561505 raw words (461380 effective words) took 1.7s, 265611 effective words/s\n",
      "2020-05-30 17:53:40,068 : INFO : EPOCH 3 - PROGRESS: at 55.19% words, 246576 words/s, in_qsize 23, out_qsize 0\n",
      "2020-05-30 17:53:40,422 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-30 17:53:40,447 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-30 17:53:40,469 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-30 17:53:40,479 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-30 17:53:40,489 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-30 17:53:40,502 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-30 17:53:40,533 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-30 17:53:40,537 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-30 17:53:40,559 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-30 17:53:40,562 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-30 17:53:40,580 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-30 17:53:40,590 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-30 17:53:40,590 : INFO : EPOCH - 3 : training on 561505 raw words (461237 effective words) took 1.6s, 297276 effective words/s\n",
      "2020-05-30 17:53:41,607 : INFO : EPOCH 4 - PROGRESS: at 55.19% words, 252128 words/s, in_qsize 22, out_qsize 1\n",
      "2020-05-30 17:53:42,097 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-30 17:53:42,111 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-30 17:53:42,117 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-30 17:53:42,129 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-30 17:53:42,203 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-30 17:53:42,205 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-30 17:53:42,210 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-30 17:53:42,211 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-30 17:53:42,242 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-30 17:53:42,246 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-30 17:53:42,257 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-30 17:53:42,260 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-30 17:53:42,261 : INFO : EPOCH - 4 : training on 561505 raw words (461383 effective words) took 1.7s, 277748 effective words/s\n",
      "2020-05-30 17:53:43,426 : INFO : EPOCH 5 - PROGRESS: at 55.19% words, 220895 words/s, in_qsize 23, out_qsize 0\n",
      "2020-05-30 17:53:43,756 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-30 17:53:43,770 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-30 17:53:43,793 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-30 17:53:43,843 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-30 17:53:43,852 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-30 17:53:43,856 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-30 17:53:43,872 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-30 17:53:43,876 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-30 17:53:43,880 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-30 17:53:43,897 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-30 17:53:43,943 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-30 17:53:43,953 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-30 17:53:43,954 : INFO : EPOCH - 5 : training on 561505 raw words (461406 effective words) took 1.7s, 275027 effective words/s\n",
      "2020-05-30 17:53:45,069 : INFO : EPOCH 6 - PROGRESS: at 55.19% words, 231235 words/s, in_qsize 23, out_qsize 0\n",
      "2020-05-30 17:53:45,495 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-30 17:53:45,501 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-30 17:53:45,543 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-30 17:53:45,548 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-30 17:53:45,556 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-30 17:53:45,562 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-30 17:53:45,583 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-30 17:53:45,603 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-30 17:53:45,612 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-30 17:53:45,614 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-30 17:53:45,635 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-30 17:53:45,662 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-30 17:53:45,662 : INFO : EPOCH - 6 : training on 561505 raw words (461337 effective words) took 1.7s, 272701 effective words/s\n",
      "2020-05-30 17:53:46,690 : INFO : EPOCH 7 - PROGRESS: at 55.19% words, 253307 words/s, in_qsize 23, out_qsize 0\n",
      "2020-05-30 17:53:47,092 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-30 17:53:47,112 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-30 17:53:47,162 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-30 17:53:47,166 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-30 17:53:47,169 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-30 17:53:47,182 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-30 17:53:47,211 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-30 17:53:47,213 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-30 17:53:47,224 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-30 17:53:47,237 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-30 17:53:47,241 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-30 17:53:47,266 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-30 17:53:47,267 : INFO : EPOCH - 7 : training on 561505 raw words (461268 effective words) took 1.6s, 292066 effective words/s\n",
      "2020-05-30 17:53:48,290 : INFO : EPOCH 8 - PROGRESS: at 51.63% words, 236954 words/s, in_qsize 21, out_qsize 2\n",
      "2020-05-30 17:53:48,711 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-30 17:53:48,773 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-30 17:53:48,775 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-30 17:53:48,813 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-30 17:53:48,842 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-30 17:53:48,844 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-30 17:53:48,866 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-30 17:53:48,886 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-30 17:53:48,898 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-30 17:53:48,918 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-30 17:53:48,921 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-30 17:53:48,929 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-30 17:53:48,929 : INFO : EPOCH - 8 : training on 561505 raw words (461765 effective words) took 1.6s, 281422 effective words/s\n",
      "2020-05-30 17:53:49,975 : INFO : EPOCH 9 - PROGRESS: at 55.19% words, 246547 words/s, in_qsize 24, out_qsize 0\n",
      "2020-05-30 17:53:50,325 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-30 17:53:50,376 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-30 17:53:50,394 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-30 17:53:50,396 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-30 17:53:50,442 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-30 17:53:50,443 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-30 17:53:50,450 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-30 17:53:50,452 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-30 17:53:50,468 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-30 17:53:50,477 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-30 17:53:50,486 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-30 17:53:50,497 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-30 17:53:50,497 : INFO : EPOCH - 9 : training on 561505 raw words (461497 effective words) took 1.6s, 297103 effective words/s\n",
      "2020-05-30 17:53:51,532 : INFO : EPOCH 10 - PROGRESS: at 55.19% words, 249541 words/s, in_qsize 24, out_qsize 0\n",
      "2020-05-30 17:53:51,915 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2020-05-30 17:53:51,919 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2020-05-30 17:53:51,936 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2020-05-30 17:53:51,946 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2020-05-30 17:53:51,955 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-05-30 17:53:51,985 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-05-30 17:53:52,006 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-05-30 17:53:52,012 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-05-30 17:53:52,014 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-05-30 17:53:52,015 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-05-30 17:53:52,035 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-05-30 17:53:52,038 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-05-30 17:53:52,039 : INFO : EPOCH - 10 : training on 561505 raw words (461382 effective words) took 1.5s, 302444 effective words/s\n",
      "2020-05-30 17:53:52,039 : INFO : training on a 5615050 raw words (4613985 effective words) took 16.4s, 281931 effective words/s\n"
     ]
    }
   ],
   "source": [
    "# train on twitter corpus\n",
    "ft_model = FastText.load(str(save_path))\n",
    "ft_model.build_vocab(processed_tweets, update=True)\n",
    "total_words = ft_model.corpus_total_words\n",
    "ft_model.train(processed_tweets, total_words=total_words, epochs=ft_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-30 17:54:16,988 : INFO : saving FastText object under ..\\embeddings\\fasttext\\twitter\\twitter_ft.model, separately None\n",
      "2020-05-30 17:54:16,988 : INFO : storing np array 'vectors' to ..\\embeddings\\fasttext\\twitter\\twitter_ft.model.wv.vectors.npy\n",
      "2020-05-30 17:54:17,128 : INFO : storing np array 'vectors_vocab' to ..\\embeddings\\fasttext\\twitter\\twitter_ft.model.wv.vectors_vocab.npy\n",
      "2020-05-30 17:54:17,254 : INFO : storing np array 'vectors_ngrams' to ..\\embeddings\\fasttext\\twitter\\twitter_ft.model.wv.vectors_ngrams.npy\n",
      "2020-05-30 17:54:20,149 : INFO : not storing attribute vectors_norm\n",
      "2020-05-30 17:54:20,150 : INFO : not storing attribute vectors_vocab_norm\n",
      "2020-05-30 17:54:20,150 : INFO : not storing attribute vectors_ngrams_norm\n",
      "2020-05-30 17:54:20,151 : INFO : not storing attribute buckets_word\n",
      "2020-05-30 17:54:20,152 : INFO : storing np array 'syn1neg' to ..\\embeddings\\fasttext\\twitter\\twitter_ft.model.trainables.syn1neg.npy\n",
      "2020-05-30 17:54:20,266 : INFO : storing np array 'vectors_vocab_lockf' to ..\\embeddings\\fasttext\\twitter\\twitter_ft.model.trainables.vectors_vocab_lockf.npy\n",
      "2020-05-30 17:54:20,375 : INFO : storing np array 'vectors_ngrams_lockf' to ..\\embeddings\\fasttext\\twitter\\twitter_ft.model.trainables.vectors_ngrams_lockf.npy\n",
      "2020-05-30 17:54:22,850 : INFO : saved ..\\embeddings\\fasttext\\twitter\\twitter_ft.model\n"
     ]
    }
   ],
   "source": [
    "ft_model.save(str(save_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
