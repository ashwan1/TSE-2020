{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('../')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import get_tokenizer, get_jaccard_from_df\n",
    "from config import Config\n",
    "from data_utils import AlbertDataGenerator, XLNetDataGenerator, BertDataGenerator, RobertaDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tjs = {}\n",
    "data_df = pd.read_csv(Config.train_path).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = np.zeros((data_df.shape[0], Config.Train.max_len))\n",
    "et = np.zeros((data_df.shape[0], Config.Train.max_len))\n",
    "g = AlbertDataGenerator(data_df).generate()\n",
    "for i, (_, y) in enumerate(g):\n",
    "    st[i, :len(y['sts'])] = y['sts']\n",
    "    et[i, :len(y['ets'])] = y['ets']\n",
    "st = np.argmax(st, axis=-1)\n",
    "et = np.argmax(et, axis=-1)\n",
    "tjs['albert'] = get_jaccard_from_df(data_df, st, et, 'albert', 'albert_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = np.zeros((data_df.shape[0], Config.Train.max_len))\n",
    "et = np.zeros((data_df.shape[0], Config.Train.max_len))\n",
    "g = XLNetDataGenerator(data_df).generate()\n",
    "for i, (_, y) in enumerate(g):\n",
    "    st[i, :len(y['sts'])] = y['sts']\n",
    "    et[i, :len(y['ets'])] = y['ets']\n",
    "st = np.argmax(st, axis=-1)\n",
    "et = np.argmax(et, axis=-1)\n",
    "tjs['xlnet'] = get_jaccard_from_df(data_df, st, et, 'xlnet', 'xlnet_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = np.zeros((data_df.shape[0], Config.Train.max_len))\n",
    "et = np.zeros((data_df.shape[0], Config.Train.max_len))\n",
    "g = BertDataGenerator(data_df).generate()\n",
    "for i, (_, y) in enumerate(g):\n",
    "    st[i, :len(y['sts'])] = y['sts']\n",
    "    et[i, :len(y['ets'])] = y['ets']\n",
    "st = np.argmax(st, axis=-1)\n",
    "et = np.argmax(et, axis=-1)\n",
    "tjs['bert'] = get_jaccard_from_df(data_df, st, et, 'bert', 'bert_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = np.zeros((data_df.shape[0], Config.Train.max_len))\n",
    "et = np.zeros((data_df.shape[0], Config.Train.max_len))\n",
    "g = BertDataGenerator(data_df).generate()\n",
    "for i, (_, y) in enumerate(g):\n",
    "    st[i, :len(y['sts'])] = y['sts']\n",
    "    et[i, :len(y['ets'])] = y['ets']\n",
    "st = np.argmax(st, axis=-1)\n",
    "et = np.argmax(et, axis=-1)\n",
    "tjs['electra'] = get_jaccard_from_df(data_df, st, et, 'bert', 'electra_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = np.zeros((data_df.shape[0], Config.Train.max_len))\n",
    "et = np.zeros((data_df.shape[0], Config.Train.max_len))\n",
    "g = RobertaDataGenerator(data_df).generate()\n",
    "for i, (_, y) in enumerate(g):\n",
    "    st[i, :len(y['sts'])] = y['sts']\n",
    "    et[i, :len(y['ets'])] = y['ets']\n",
    "st = np.argmax(st, axis=-1)\n",
    "et = np.argmax(et, axis=-1)\n",
    "tjs['roberta'] = get_jaccard_from_df(data_df, st, et, 'roberta', 'roberta_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model type</th>\n",
       "      <th>Jaccard score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>albert</td>\n",
       "      <td>0.956906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xlnet</td>\n",
       "      <td>0.956679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bert</td>\n",
       "      <td>0.975771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>electra</td>\n",
       "      <td>0.975771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roberta</td>\n",
       "      <td>0.959700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model type  Jaccard score\n",
       "0     albert       0.956906\n",
       "1      xlnet       0.956679\n",
       "2       bert       0.975771\n",
       "3    electra       0.975771\n",
       "4    roberta       0.959700"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tj_df = pd.DataFrame({\n",
    "    'model type': list(tjs.keys()),\n",
    "    'Jaccard score': list(tjs.values())\n",
    "})\n",
    "tj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('albert')\n",
    "sentiment_ids = {'positive': 2221, 'negative': 3682, 'neutral': 8387}\n",
    "def create_albert_data(text, selected_text, sentiment):\n",
    "    text = text.lower()\n",
    "    selected_text = selected_text.lower()\n",
    "    text = ' '.join(text.split())\n",
    "    selected_text = ' '.join(selected_text.split())\n",
    "    # find the intersection between text and selected text\n",
    "    idx_start = text.find(selected_text)\n",
    "    \n",
    "\n",
    "    text_tokens = tokenizer.tokenize(text)\n",
    "    selected_text_tokens = tokenizer.tokenize(selected_text)\n",
    "    chars = np.zeros((len(''.join(text_tokens))))\n",
    "    chars[idx_start:idx_start + len(''.join(selected_text_tokens))] = 1\n",
    "    offsets = []\n",
    "    idx = 0\n",
    "    for t in text_tokens:\n",
    "        len_t = len(t)\n",
    "        offsets.append((idx, idx + len_t))\n",
    "        idx += len_t\n",
    "\n",
    "    # compute targets\n",
    "    target_idx = []\n",
    "    for i, (o1, o2) in enumerate(offsets):\n",
    "        if sum(chars[o1: o2]) > 0:\n",
    "            target_idx.append(i)\n",
    "\n",
    "    start_tokens = target_idx[0]\n",
    "    end_tokens = target_idx[-1]\n",
    "\n",
    "    input_ids_orig = tokenizer.encode(text, add_special_tokens=False)\n",
    "    input_ids = [2] + input_ids_orig + [3] + [sentiment_ids[sentiment]] + [3]\n",
    "    token_type_ids = [0] * (len(input_ids_orig) + 2) + [1, 1]\n",
    "    attention_mask = [1] * (len(input_ids_orig) + 4)\n",
    "    np_start_tokens = np.zeros((len(input_ids)), dtype='int')\n",
    "    np_start_tokens[start_tokens] = 1\n",
    "    a = np.argmax(np_start_tokens, axis=-1)\n",
    "    np_end_tokens = np.zeros((len(input_ids)), dtype='int')\n",
    "    np_end_tokens[end_tokens] = 1\n",
    "    b = np.argmax(np_end_tokens, axis=-1)\n",
    "    start_tokens = np_start_tokens.tolist()\n",
    "    end_tokens = np_end_tokens.tolist()\n",
    "    encoded_text = tokenizer.tokenize(text)\n",
    "    pred_selected_text = tokenizer.convert_tokens_to_string(encoded_text[a: b + 1])\n",
    "    return ({'ids': input_ids, 'att': attention_mask, 'tti': token_type_ids},\n",
    "            {'start_token': a, 'end_token': b},\n",
    "            {'sts': start_tokens, 'ets': end_tokens},\n",
    "            {'tok_text': tokenizer.tokenize(text), 'pred_selected_text': pred_selected_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'ids': [2, 31, 1, 79, 1875, 9, 3, 3682, 3],\n",
       "  'att': [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'tti': [0, 0, 0, 0, 0, 0, 0, 1, 1]},\n",
       " {'start_token': 0, 'end_token': 4},\n",
       " {'sts': [1, 0, 0, 0, 0, 0, 0, 0, 0], 'ets': [0, 0, 0, 0, 1, 0, 0, 0, 0]},\n",
       " {'tok_text': ['▁i', '`', 'm', '▁sorry', '.'],\n",
       "  'pred_selected_text': 'i`m sorry.'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '  I`m sorry.'\n",
    "selected_text = 'I`m sorry.'\n",
    "sentiment = 'negative'\n",
    "create_albert_data(text, selected_text, sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_xlnet = get_tokenizer('xlnet')\n",
    "sentiment_ids = {'positive': 1654, 'negative': 2981, 'neutral': 9201}\n",
    "def create_albert_data(text, selected_text, sentiment):\n",
    "    text = text.lower()\n",
    "    selected_text = selected_text.lower()\n",
    "    # find overlap\n",
    "    text = ' '.join(text.split())\n",
    "    selected_text = ' '.join(selected_text.split())\n",
    "    # find the intersection between text and selected text\n",
    "    idx_start = text.find(selected_text)\n",
    "\n",
    "    # calculate offsets\n",
    "    text_tokens = tokenizer_xlnet.tokenize(text)\n",
    "    selected_text_tokens = tokenizer_xlnet.tokenize(selected_text)\n",
    "    chars = np.zeros((len(''.join(text_tokens))))\n",
    "    chars[idx_start:idx_start + len(''.join(selected_text_tokens))] = 1\n",
    "    offsets = []\n",
    "    idx = 0\n",
    "    for t in text_tokens:\n",
    "        len_t = len(t)\n",
    "        offsets.append((idx, idx + len_t))\n",
    "        idx += len_t\n",
    "\n",
    "    # compute targets\n",
    "    target_idx = []\n",
    "    for i, (o1, o2) in enumerate(offsets):\n",
    "        if sum(chars[o1: o2]) > 0:\n",
    "            target_idx.append(i)\n",
    "\n",
    "    start_tokens = target_idx[0]\n",
    "    end_tokens = target_idx[-1]\n",
    "\n",
    "    input_ids_orig = tokenizer_xlnet.encode(text, add_special_tokens=False)\n",
    "    input_ids = input_ids_orig + [4] + [sentiment_ids[sentiment]] + [4, 3]\n",
    "    token_type_ids = [0] * (len(input_ids_orig) + 1) + [1, 1] + [2]\n",
    "    attention_mask = [1] * (len(input_ids_orig) + 4)\n",
    "    np_start_tokens = np.zeros((len(input_ids)), dtype='int')\n",
    "    np_start_tokens[start_tokens] = 1\n",
    "    a = np.argmax(np_start_tokens, axis=-1)\n",
    "    np_end_tokens = np.zeros((len(input_ids)), dtype='int')\n",
    "    np_end_tokens[end_tokens] = 1\n",
    "    b = np.argmax(np_end_tokens, axis=-1)\n",
    "    start_tokens = np_start_tokens.tolist()\n",
    "    end_tokens = np_end_tokens.tolist()\n",
    "    encoded_text = tokenizer_xlnet.tokenize(text)\n",
    "    pred_selected_text = tokenizer_xlnet.convert_tokens_to_string(encoded_text[a: b + 1])\n",
    "    return ({'ids': input_ids, 'att': attention_mask, 'tti': token_type_ids},\n",
    "            {'start_token': a, 'end_token': b},\n",
    "            {'sts': start_tokens, 'ets': end_tokens},\n",
    "            {'tok_text': tokenizer_xlnet.tokenize(text), 'pred_selected_text': pred_selected_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'ids': [94, 3346, 950, 27, 926, 25550, 599, 31, 17691, 4, 2981, 4, 3],\n",
       "  'att': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "  'tti': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2]},\n",
       " {'start_token': 5, 'end_token': 5},\n",
       " {'sts': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  'ets': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]},\n",
       " {'tok_text': ['▁my',\n",
       "   '▁sharp',\n",
       "   'ie',\n",
       "   '▁is',\n",
       "   '▁running',\n",
       "   '▁dangerously',\n",
       "   '▁low',\n",
       "   '▁on',\n",
       "   '▁ink'],\n",
       "  'pred_selected_text': 'dangerously'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'My Sharpie is running DANGERously low on ink'\n",
    "selected_text = 'DANGERously'\n",
    "sentiment = 'negative'\n",
    "create_albert_data(text, selected_text, sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁my',\n",
       " '▁sharp',\n",
       " 'ie',\n",
       " '▁is',\n",
       " '▁running',\n",
       " '▁dangerously',\n",
       " '▁low',\n",
       " '▁on',\n",
       " '▁ink']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_xlnet.tokenize('My Sharpie is running DANGERously low on ink'.lower())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
