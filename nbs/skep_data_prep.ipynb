{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "os.sys.path.append(str(Path('../')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob, Word\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.keras import TqdmCallback\n",
    "from ftfy import fix_text\n",
    "import re\n",
    "import warnings\n",
    "import random\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from utils import get_tokenizer\n",
    "from models.roberta import get_roberta_for_skep\n",
    "from custom_callbacks.warmup_cosine_decay import WarmUpCosineDecayScheduler\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = None\n",
    "warnings.filterwarnings('ignore')\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMAIL_REGEX = re.compile(\n",
    "    r\"(?:^|(?<=[^\\w@.)]))([\\w+-](\\.(?!\\.))?)*?[\\w+-]@(?:\\w-?)*?\\w+(\\.([a-z]{2,})){1,3}(?:$|(?=\\b))\",\n",
    "    flags=re.IGNORECASE | re.UNICODE,\n",
    ")\n",
    "\n",
    "PHONE_REGEX = re.compile(\n",
    "    r\"(?:^|(?<=[^\\w)]))(\\+?1[ .-]?)?(\\(?\\d{3}\\)?[ .-]?)?(\\d{3}[ .-]?\\d{4})(\\s?(?:ext\\.?|[#x-])\\s?\\d{2,6})?(?:$|(?=\\W))\"\n",
    ")\n",
    "\n",
    "MULTI_WHITESPACE_TO_ONE_REGEX = re.compile(r\"\\s+\")\n",
    "\n",
    "URL_REGEX = re.compile(\n",
    "    r\"(?:^|(?<![\\w\\/\\.]))\"\n",
    "    # protocol identifier\n",
    "    # r\"(?:(?:https?|ftp)://)\"  <-- alt?\n",
    "    r\"(?:(?:https?:\\/\\/|ftp:\\/\\/|www\\d{0,3}\\.))\"\n",
    "    # user:pass authentication\n",
    "    r\"(?:\\S+(?::\\S*)?@)?\" r\"(?:\"\n",
    "    # IP address exclusion\n",
    "    # private & local networks\n",
    "    r\"(?!(?:10|127)(?:\\.\\d{1,3}){3})\"\n",
    "    r\"(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})\"\n",
    "    r\"(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})\"\n",
    "    # IP address dotted notation octets\n",
    "    # excludes loopback network 0.0.0.0\n",
    "    # excludes reserved space >= 224.0.0.0\n",
    "    # excludes network & broadcast addresses\n",
    "    # (first & last IP address of each class)\n",
    "    r\"(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])\"\n",
    "    r\"(?:\\.(?:1?\\d{1,2}|2[0-4]\\d|25[0-5])){2}\"\n",
    "    r\"(?:\\.(?:[1-9]\\d?|1\\d\\d|2[0-4]\\d|25[0-4]))\"\n",
    "    r\"|\"\n",
    "    # host name\n",
    "    r\"(?:(?:[a-z\\\\u00a1-\\\\uffff0-9]-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)\"\n",
    "    # domain name\n",
    "    r\"(?:\\.(?:[a-z\\\\u00a1-\\\\uffff0-9]-?)*[a-z\\\\u00a1-\\\\uffff0-9]+)*\"\n",
    "    # TLD identifier\n",
    "    r\"(?:\\.(?:[a-z\\\\u00a1-\\\\uffff]{2,}))\" r\")\"\n",
    "    # port number\n",
    "    r\"(?::\\d{2,5})?\"\n",
    "    # resource path\n",
    "    r\"(?:\\/[^\\)\\]\\}\\s]*)?\",\n",
    "    # r\"(?:$|(?![\\w?!+&\\/\\)]))\",\n",
    "    # @jfilter: I removed the line above from the regex because I don't understand what it is used for, maybe it was useful?\n",
    "    # But I made sure that it does not include ), ] and } in the URL.\n",
    "    flags=re.UNICODE | re.IGNORECASE,\n",
    ")\n",
    "\n",
    "strange_double_quotes = [\n",
    "    \"«\",\n",
    "    \"‹\",\n",
    "    \"»\",\n",
    "    \"›\",\n",
    "    \"„\",\n",
    "    \"“\",\n",
    "    \"‟\",\n",
    "    \"”\",\n",
    "    \"❝\",\n",
    "    \"❞\",\n",
    "    \"❮\",\n",
    "    \"❯\",\n",
    "    \"〝\",\n",
    "    \"〞\",\n",
    "    \"〟\",\n",
    "    \"＂\",\n",
    "]\n",
    "strange_single_quotes = [\"‘\", \"‛\", \"’\", \"❛\", \"❜\", \"`\", \"´\", \"‘\", \"’\"]\n",
    "\n",
    "DOUBLE_QUOTE_REGEX = re.compile(\"|\".join(strange_double_quotes))\n",
    "SINGLE_QUOTE_REGEX = re.compile(\"|\".join(strange_single_quotes))\n",
    "HASHTAG_REGEX = re.compile(\"(?:^|\\s)[＃#]{1}(\\w+)\", re.UNICODE)\n",
    "MENTION_REGEX = re.compile(\"(?:^|\\s)[＠ @]{1}([^\\s#<>[\\]|{}]+)\", re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_strange_quotes(text):\n",
    "    text = SINGLE_QUOTE_REGEX.sub(\"'\", text)\n",
    "    text = DOUBLE_QUOTE_REGEX.sub('\"', text)\n",
    "    return text\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    text = MULTI_WHITESPACE_TO_ONE_REGEX.sub(\" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def replace_urls(text, replace_with=\"<URL>\"):\n",
    "    return URL_REGEX.sub(replace_with, text)\n",
    "\n",
    "def replace_emails(text, replace_with=\"<EMAIL>\"):\n",
    "    return EMAIL_REGEX.sub(replace_with, text)\n",
    "\n",
    "def replace_phone_numbers(text, replace_with=\"<PHONE>\"):\n",
    "    return PHONE_REGEX.sub(replace_with, text)\n",
    "\n",
    "def replace_hashtag(text, replace_with=''):\n",
    "    return HASHTAG_REGEX.sub(replace_with, text)\n",
    "\n",
    "def replace_mentions(text, replace_with=''):\n",
    "    return MENTION_REGEX.sub(replace_with, text)\n",
    "\n",
    "def clean_text_for_G(text):\n",
    "    text = str(text)\n",
    "    text = fix_text(text)\n",
    "    text = fix_strange_quotes(text)\n",
    "    text = replace_urls(text, replace_with='')\n",
    "    text = replace_emails(text, replace_with='')\n",
    "    text = replace_phone_numbers(text, replace_with='')\n",
    "    text = replace_hashtag(text)\n",
    "    text = replace_mentions(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    text = strip_punctuation(text)\n",
    "    return text.lower()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    text = fix_text(text)\n",
    "    text = fix_strange_quotes(text)\n",
    "    text = replace_hashtag(text)\n",
    "    text = replace_mentions(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data_paths = [\n",
    "    Path('../../full-corpus.csv'), \n",
    "    Path('../../TextEmotion.csv'), \n",
    "    Path('../data/test.csv'),\n",
    "    Path('../data/train.csv'),\n",
    "    Path('../data/validation.csv')\n",
    "]\n",
    "data_columns = [\n",
    "    'TweetText',\n",
    "    'content',\n",
    "    'text',\n",
    "    'text',\n",
    "    'text'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5add637ed94814851efd0d3e0bdc37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3bcc34f3544165974c42fc6779e1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cleaning full-corpus.csv', max=5113.0, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f180de48e67a4f9094326086203dec07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cleaning TextEmotion.csv', max=40000.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14971778cdd748da8707f5933a788cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cleaning test.csv', max=3534.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb66205a0fc479480ed7c667066ab37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cleaning train.csv', max=21983.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a58a3b115745259231a7f9f4c126ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cleaning validation.csv', max=5497.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_tweets = []\n",
    "for i, data_path in enumerate(tqdm(tweet_data_paths)):\n",
    "    df: pd.DataFrame = pd.read_csv(data_path)\n",
    "    df.dropna(inplace=True)\n",
    "    tqdm.pandas(desc=f'Cleaning {data_path.name}')\n",
    "    df[data_columns[i]] = df[data_columns[i]].progress_apply(lambda x: clean_text_for_G(x))\n",
    "    processed_tweets += df[data_columns[i]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['now swype iphone crack  iphone', 'adding carrier support iphone 4s  just announced ', 'hilarious video   guy duet  s siri  pretty sums love affair ', 'easy switch iphone  see ya ', 'i realized reason i got twitter ios5 thanks']\n"
     ]
    }
   ],
   "source": [
    "print(processed_tweets[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['beds',\n",
       "  'carnisada',\n",
       "  'rejection',\n",
       "  'yrold',\n",
       "  'marijuana',\n",
       "  'beat',\n",
       "  'okasan',\n",
       "  'cuz',\n",
       "  'retailer',\n",
       "  'wut'],\n",
       " 35726)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = []\n",
    "vocab = list(set([t for pt in processed_tweets for t in pt.split()]))\n",
    "vocab[:10], len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f897d7c3cd7147e59ef08fa4396f9305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=76127.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "G = {}\n",
    "for tweet in tqdm(processed_tweets):\n",
    "    blob = TextBlob(tweet)\n",
    "    assessments = blob.sentiment_assessments.assessments\n",
    "    for assessment in assessments:\n",
    "        p = assessment[1]\n",
    "        if p > 0:\n",
    "            p = 'positive'\n",
    "        elif p < 0:\n",
    "            p = 'negative'\n",
    "        else:\n",
    "            p = 'neutral'\n",
    "        w = Word(assessment[0][0])\n",
    "        w = w.lemmatize()\n",
    "        G[w] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5a059afbe840a5a1b1c501b1239578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f238ecb04f674c90873db904200d2b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cleaning full-corpus.csv', max=5113.0, style=ProgressStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3c4f3973c641af99cf61580cc77097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cleaning TextEmotion.csv', max=40000.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f000a835fe84941878084626c55d6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cleaning test.csv', max=3534.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbce9e7d2b3442e8acd8fb5d9076712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cleaning train.csv', max=21983.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9d4cb062bc47dca04afadc006fbea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Cleaning validation.csv', max=5497.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processed_tweets = []\n",
    "for i, data_path in enumerate(tqdm(tweet_data_paths)):\n",
    "    df: pd.DataFrame = pd.read_csv(data_path)\n",
    "    df.dropna(inplace=True)\n",
    "    tqdm.pandas(desc=f'Cleaning {data_path.name}')\n",
    "    df[data_columns[i]] = df[data_columns[i]].progress_apply(lambda x: clean_text(x))\n",
    "    processed_tweets += df[data_columns[i]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1e8811b58d460687564439e3843b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=76127.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>word</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i'm so getting the &lt;mask&gt;</td>\n",
       "      <td>cold</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;mask&gt; finished my marketing project only took me 7 hours..now just hanging out and relaxing</td>\n",
       "      <td>finally</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>meaning to email you for months. your pre-reunion party must be coming up soon. &lt;mask&gt; wish i could be there. &gt;140 char soon</td>\n",
       "      <td>really</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>an iphone app came out a &lt;mask&gt; months back called zemote, bumped my domain zemote.com out of the spot</td>\n",
       "      <td>few</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>im an &lt;mask&gt; fan of **** magazine and i love your magazines</td>\n",
       "      <td>avid</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91614</th>\n",
       "      <td>&lt;mask&gt; it's still not the same going to have a look though.</td>\n",
       "      <td>awww</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91615</th>\n",
       "      <td>wow.. tomorrow and then it's over. i'll never see some of those people again. it's &lt;mask&gt; of sad.</td>\n",
       "      <td>kind</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91616</th>\n",
       "      <td>winding down, &lt;mask&gt; having a low key day.</td>\n",
       "      <td>love</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91617</th>\n",
       "      <td>hey and this is &lt;mask&gt; dumb. http://t.co/kubkpo0t</td>\n",
       "      <td>pretty</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91618</th>\n",
       "      <td>im so jealous. i &lt;mask&gt; an octo drive</td>\n",
       "      <td>want</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91619 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                               text  \\\n",
       "0                                                                                                         i'm so getting the <mask>   \n",
       "1                                      <mask> finished my marketing project only took me 7 hours..now just hanging out and relaxing   \n",
       "2      meaning to email you for months. your pre-reunion party must be coming up soon. <mask> wish i could be there. >140 char soon   \n",
       "3                            an iphone app came out a <mask> months back called zemote, bumped my domain zemote.com out of the spot   \n",
       "4                                                                       im an <mask> fan of **** magazine and i love your magazines   \n",
       "...                                                                                                                             ...   \n",
       "91614                                                                   <mask> it's still not the same going to have a look though.   \n",
       "91615                             wow.. tomorrow and then it's over. i'll never see some of those people again. it's <mask> of sad.   \n",
       "91616                                                                                    winding down, <mask> having a low key day.   \n",
       "91617                                                                             hey and this is <mask> dumb. http://t.co/kubkpo0t   \n",
       "91618                                                                                         im so jealous. i <mask> an octo drive   \n",
       "\n",
       "          word  polarity  \n",
       "0         cold  negative  \n",
       "1      finally  negative  \n",
       "2       really  positive  \n",
       "3          few  negative  \n",
       "4         avid  positive  \n",
       "...        ...       ...  \n",
       "91614     awww  positive  \n",
       "91615     kind  positive  \n",
       "91616     love  positive  \n",
       "91617   pretty  positive  \n",
       "91618     want  positive  \n",
       "\n",
       "[91619 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = []\n",
    "sentiment_words = []\n",
    "sentiment_polarities = []\n",
    "for tweet in tqdm(processed_tweets):\n",
    "    words = tweet.split()\n",
    "    if len(words) > 2: \n",
    "        for i, word in enumerate(words):\n",
    "            try:\n",
    "                sentiment = G[Word(word).lemmatize()]\n",
    "                words[i] = '<mask>'\n",
    "                texts.append(' '.join(words))\n",
    "                sentiment_words.append(word)\n",
    "                sentiment_polarities.append(sentiment)\n",
    "                words[i] = word\n",
    "            except KeyError:\n",
    "                continue\n",
    "skep_data = pd.DataFrame({\n",
    "    'text': texts,\n",
    "    'word': sentiment_words,\n",
    "    'polarity': sentiment_polarities\n",
    "})\n",
    "skep_data = skep_data.sample(frac=1).reset_index(drop=True)\n",
    "skep_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1711"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set(skep_data.word.tolist() + random.sample(vocab, 500)))\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_idx = {v: i for i, v in enumerate(vocab)}\n",
    "sentiment_2_idx = {'neutral': 0, 'positive': 1, 'negative': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([347, 1631, 1515, 1111, 884], [2, 2, 1, 2, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = [word_2_idx[w] for w in skep_data.word.tolist()]\n",
    "y2 = [sentiment_2_idx[s] for s in skep_data.polarity.tolist()]\n",
    "y1[:5], y2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " (91619, 1711),\n",
       " array([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.]], dtype=float32),\n",
       " (91619, 3))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = keras.utils.to_categorical(y1, num_classes=vocab_size)\n",
    "y2 = keras.utils.to_categorical(y2, num_classes=3)\n",
    "y1[:5], y1.shape, y2[:5], y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('roberta')\n",
    "mask_token_id = tokenizer.get_vocab()['<mask>']\n",
    "MAX_LEN = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb836c81b5334199a5860f267c2dc1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=91619.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[    0,   939,   437,    98,   562,     5,  1437, 50264,  1437,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1],\n",
       "        [    0,  1437, 50264,  1550,   127,  2474,   695,   129,   362,\n",
       "           162,   262,   722,  7586,  8310,    95,  7209,    66,     8,\n",
       "         19448,     2,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1],\n",
       "        [    0,  3099,     7,  1047,    47,    13,   377,     4,   110,\n",
       "          1198,    12,   241, 18988,   537,   531,    28,   567,    62,\n",
       "          1010,     4,  1437, 50264,  2813,   939,   115,    28,    89,\n",
       "             4,  8061, 14753, 16224,  1010,     2,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1]]),\n",
       " (91619, 96))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_text = skep_data.shape[0]\n",
    "input_ids = np.ones((num_text, MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((num_text, MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((num_text, MAX_LEN),dtype='int32')\n",
    "for i, text in enumerate(tqdm(skep_data.text.tolist())):\n",
    "    text = ' ' + ' '.join(text.split())\n",
    "    texts = text.split('<mask>')\n",
    "    enc = [0] + tokenizer.encode(texts[0]).ids + [mask_token_id] + tokenizer.encode(texts[1]).ids + [2]\n",
    "    input_ids[i, :len(enc)] = enc\n",
    "    attention_mask[i, :len(enc)] = 1\n",
    "input_ids[:3], input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "ids (InputLayer)                [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att (InputLayer)                [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tti (InputLayer)                [(None, 96)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_roberta_model (TFRobertaMode ((None, 96, 768), (N 124645632   ids[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 768)          0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 768)          0           tf_roberta_model[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "sw (Dense)                      (None, 1711)         1315759     global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "sp (Dense)                      (None, 3)            2307        global_average_pooling1d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 125,963,698\n",
      "Trainable params: 125,963,698\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_roberta_for_skep(vocab_size, 5e-5)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(str(Config.Train.checkpoint_dir / f'skep/weights.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d58667d6e344ef8e042f46b5d8bcfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "\n",
      "Epoch 00011: val_loss improved from inf to 2.70547, saving model to ..\\checkpoints\\skep\\weights.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.70547 to 2.69304, saving model to ..\\checkpoints\\skep\\weights.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.69304\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.69304\n",
      "Epoch 00014: early stopping\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f76c452a48>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbs = [\n",
    "    # keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1, factor=0.3),\n",
    "#     WarmUpCosineDecayScheduler(6e-5, 1500, warmup_steps=300, hold_base_rate_steps=200, verbose=0),\n",
    "    keras.callbacks.EarlyStopping(patience=2, verbose=1, restore_best_weights=True, baseline=2.72400),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        str(Config.Train.checkpoint_dir / f'skep/weights.h5'),\n",
    "        verbose=1, save_best_only=True, save_weights_only=True),\n",
    "    TqdmCallback()\n",
    "]\n",
    "model.fit([input_ids, attention_mask, token_type_ids], [y1, y2], epochs=50, verbose=0, \n",
    "          validation_split=0.2, callbacks=cbs, initial_epoch=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
